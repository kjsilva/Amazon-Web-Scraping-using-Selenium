{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Web Scraping using Selenium"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from random import sample\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from selenium.common import exceptions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "selenium.__version__\n",
    "print(\"done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SET DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"\")\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Begin the scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Automated Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install() , options=options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to Core Website/Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to Core Website/Link Where you want to scrape the category and product links\n",
    "\n",
    "driver.get('https://www.amazon.com/Best-Sellers-Electronics-Computers-Tablets/zgbs/electronics/13896617011/ref=zg_bs_nav_electronics_2_541966')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Categories (Computers, Tablets, Desktop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Categories \n",
    "\n",
    "EL_sidebar = driver.find_elements(By.XPATH,f\"//div[@role='group']\")\n",
    "\n",
    "\n",
    "if len(EL_sidebar)==1:\n",
    "    EL_sidebar = EL_sidebar[0]\n",
    "else:\n",
    "    print('Tag Mismatch')\n",
    "\n",
    "catlist = EL_sidebar.find_elements(By.XPATH,f\".//a\")\n",
    "category_list = []\n",
    "link_list = []\n",
    "for i in catlist:\n",
    "    category_list.append(i.text) \n",
    "    link_list.append(i.get_attribute('href'))\n",
    "    \n",
    "print(category_list)\n",
    "print(link_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Scraping Product Links (review_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty Array Variable\n",
    "FINAL = []\n",
    "\n",
    "# Start Scraping\n",
    "\n",
    "for cat_pick,link_pick in tqdm(zip(category_list,link_list)):\n",
    "    \n",
    "    driver.get(link_pick)\n",
    "\n",
    "\n",
    "    # Scroll To Load all the data \n",
    "    y = 0\n",
    "    for timer in range(0,15):\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "        y += 1500\n",
    "        time.sleep(.3)\n",
    "    time.sleep(2)\n",
    "    # Scrape All The Items \n",
    "    items = driver.find_elements(By.XPATH,f\"//div[@id='gridItemRoot']\")\n",
    "    for item in items:\n",
    "        name = item.find_elements(By.XPATH,f\".//div[@class='_cDEzb_p13n-sc-css-line-clamp-3_g3dy1']\")\n",
    "        if len(name) == 1:\n",
    "            name = name[0].text\n",
    "\n",
    "        rank = item.find_elements(By.XPATH,f\".//span[@class='zg-bdg-text']\")\n",
    "        if len(rank) == 1:\n",
    "            rank = rank[0].text\n",
    "        \n",
    "        price = item.find_elements(By.XPATH,f\".//span[@class='a-size-base a-color-price']\")\n",
    "        if len(price) == 1:\n",
    "            price = price[0].text\n",
    "\n",
    "        offer = item.find_elements(By.XPATH,f\".//span[@class='a-color-secondary']//span[@class='a-size-base']\")\n",
    "        if len(offer) == 1:\n",
    "            offer = offer[0].text\n",
    "            \n",
    "\n",
    "        stareview = item.find_elements(By.XPATH,f\".//div[@class='a-icon-row']//a[@class='a-link-normal']\")\n",
    "        if len(stareview) == 1:\n",
    "            reviews = stareview[0].text\n",
    "            stars = stareview[0].get_attribute('title')\n",
    "            review_page = stareview[0].get_attribute('href')\n",
    "\n",
    "        meta = {\n",
    "            'name' : name,\n",
    "            'rank' : rank,\n",
    "            'reviews' : reviews,\n",
    "            'stars' : stars,\n",
    "            'review_page' : review_page,\n",
    "            'cat' : cat_pick,\n",
    "            'print' : price,\n",
    "            'offer' : offer,\n",
    "        }\n",
    "\n",
    "        FINAL.append(meta)\n",
    "print(\"Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Product Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINALdf = pd.DataFrame(FINAL)\n",
    "FINALdf.to_csv(\"D:/desktop/3-2-23 Testing/links.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the reviews from the scraped product links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 (SCRAPING - Whole Computer and Tablets Category Links)\n",
    "\n",
    "df = pd.read_csv(\"D:/desktop/3-2-23 Testing/links.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 (CHANGE AFTER EACH PRODUCT - CHOOSE SPECIFIC PRODUCT)\n",
    "\n",
    "dfnew = df.loc[(df['rank'] == '#1') & (df['cat'] == 'Desktops')]\n",
    "dfnew = dfnew.reset_index(drop=True)\n",
    "dfnew = dfnew.iloc[:,1:]\n",
    "dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 (START SCRAPING OF REVIEWSTAR AND REVIEWBODY\n",
    "# CREATE EMPTY ARRAY VARIABLE\n",
    "\n",
    "FINALFINAL = []\n",
    "\n",
    "for i in tqdm(range(len(dfnew))):\n",
    "    reviewpage = dfnew.loc[i]['review_page']\n",
    "    name = dfnew.loc[i]['name']\n",
    "    categoryRank = dfnew.loc[i]['rank']\n",
    "    noRatings = dfnew.loc[i]['reviews']\n",
    "    stars = dfnew.loc[i]['stars'].split(' ')[0]\n",
    "    category = dfnew.loc[i]['cat']\n",
    "    cost = dfnew.loc[i]['print']\n",
    "    \n",
    "    driver.get(reviewpage)\n",
    "    \n",
    "    product = driver.find_elements(By.XPATH,f\"//a[@data-hook='product-link']\")\n",
    "\n",
    "    product_name = ''\n",
    "    product_url = ''\n",
    "    \n",
    "    if len(product) == 1:\n",
    "        product_name = product[0].text\n",
    "        product_url = product[0].get_attribute('href')  \n",
    "\n",
    "    number_sang_reviews = ''\n",
    "    number_sang_reviews = driver.find_elements(By.XPATH,f\"//div[@data-hook='cr-filter-info-review-rating-count']\")\n",
    "    number_sang_reviews = number_sang_reviews[0].text\n",
    "    number_sang_reviews = number_sang_reviews.split(\"ratings, \")\n",
    "    number_sang_reviews = number_sang_reviews[1]\n",
    "    number_sang_reviews = re.sub(r'[^0-9]', '', number_sang_reviews)\n",
    "    number_sang_reviews = int(number_sang_reviews)\n",
    "\n",
    "    libot_count = number_sang_reviews//10\n",
    "    print(libot_count)\n",
    "\n",
    "    libot_count = 2\n",
    "    \n",
    "    for libot in range(libot_count):\n",
    "        time.sleep(2)\n",
    "        reviews = driver.find_elements(By.XPATH,f\"//div[@data-hook='review']\")\n",
    "        nextpage = driver.find_element(By.CLASS_NAME, 'a-last')\n",
    "        REVIEWBODY = []\n",
    "        REVIEWSTAR = []\n",
    "        REVIEWPAGEURL = []\n",
    "        USERNAME = []\n",
    "        VERIFICATION = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            review_body = ''\n",
    "            review_star = ''\n",
    "            review_page_url = ''\n",
    "            review_username = ''\n",
    "            review_verification = ''\n",
    "\n",
    "\n",
    "            review_page_url = driver.current_url\n",
    "\n",
    "\n",
    "            review_body_el = review.find_elements(By.XPATH,f\".//span[@data-hook='review-body']\")\n",
    "\n",
    "            if len(review_body_el)== 1:\n",
    "                review_body = review_body_el[0].text\n",
    "\n",
    "\n",
    "            review_star_el = review.find_elements(By.XPATH,f\".//i[@data-hook='review-star-rating']\")\n",
    "\n",
    "            if len(review_star_el)== 1:\n",
    "                review_star = review_star_el[0].get_attribute('class')\n",
    "\n",
    "         \n",
    "            review_username_el = review.find_elements(By.XPATH,f\".//span[@class='a-profile-name']\")\n",
    "\n",
    "            if len(review_username_el)== 1:\n",
    "                review_username = review_username_el[0].text\n",
    "\n",
    "            review_verification_el = review.find_elements(By.XPATH,f\".//span[@data-hook='avp-badge']\")\n",
    "\n",
    "            if len(review_verification_el)== 1:\n",
    "                review_verification = review_verification_el[0].text\n",
    "\n",
    "            REVIEWBODY.append({\n",
    "                'review_body':review_body\n",
    "            })\n",
    "            \n",
    "            REVIEWSTAR.append({\n",
    "                'review_star':review_star,\n",
    "            })\n",
    "            USERNAME.append({\n",
    "                'review_username':review_username,\n",
    "            })\n",
    "            VERIFICATION.append({\n",
    "                'review_verification':review_verification,\n",
    "            })\n",
    "\n",
    "            newRow = {\n",
    "                'USERNAME':USERNAME,\n",
    "                'VERIFICATION':VERIFICATION,\n",
    "                'reviewpage':reviewpage,\n",
    "                'name':name,\n",
    "                'categoryRank':categoryRank,\n",
    "                'category':category,\n",
    "                'cost':cost,\n",
    "                'REVIEWBODY':REVIEWBODY,\n",
    "                'REVIEWSTAR':REVIEWSTAR,\n",
    "                'product_url':product_url,\n",
    "                'product_name':product_name,\n",
    "                'review_page_url':review_page_url,\n",
    "                'no_of_reviews':number_sang_reviews\n",
    "            }\n",
    "\n",
    "        FINALFINAL.append(newRow)\n",
    "\n",
    "\n",
    "        FINALFINAL_DF = pd.DataFrame(FINALFINAL)\n",
    "\n",
    "\n",
    "        FINALFINAL_DF.to_csv(\"D:/desktop/3-2-23 Testing/scraped-reviews.csv\")\n",
    "        time.sleep(2)\n",
    "        nextpage.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the glob() function to list only the csv files from the working directory.\n",
    "#list all csv files only\n",
    "\n",
    "csv_files = glob.glob(\"C:/Users/DVF2/Desktop/Electronics 11-9-22/1pm\" + \"/*.csv\")\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE DATAFRAME/CREATE EMPTY DATAFRAME\n",
    "df_append = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "            df_temp = pd.read_csv(file)\n",
    "            df_append = df_append.append(df_temp, ignore_index=True)\n",
    "sample_reviews = df_append"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fcb0c6fddb345c2783bac4cadddaa714643fe378b8d7fda149a3db844527b66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
